{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Project! **[ANSWER SHEET]**\n",
    "\n",
    "##### How to work through this project:\n",
    "- Go cell by cell and finish the marked #TODO's\n",
    "- You don't need to touch the code marked between the `#---------#`. Those are puzzle pieces that your code will fit into!\n",
    "    - However, I **STRONGLY** encourage you to understand every single line between those blocks. They are essential!\n",
    "    - It is crucial that your variable names are what we expect them to be, or the puzzle pieces won't fit.\n",
    "- Tutorials/helpful information will be placed in the `.md` cells above the \"work\" cells. Consult them if you are stuck.\n",
    "- If you REALLY cannot find the correct code to make the cell run, consult the `[proj]-ans.ipynb`.\n",
    "- The final product (what we expect to see if you run all the cells consecutively) will be placed in the `answers/` directory.\n",
    "    - Chances are your output won't be the exact same (stochasticity!) but it should be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get used to these imports!\n",
    "#----------------------------------------------------------------#\n",
    "#To install: pip install numpy\n",
    "import numpy as np \n",
    "#To install: pip install matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "#To install: pip install torch (not GPU compatible)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#To install: pip install gymnasium[classic_control]\n",
    "import gymnasium as gym\n",
    "\n",
    "# No pip install needed\n",
    "from IPython import display\n",
    "import random\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be implementing DQN for Cartpole, which is a common RL benchmark from OpenAI! First, lets visualize the environment that our Deep Q-Learnign Network will operate in. You can read more about cartpole [here](https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288)\n",
    "\n",
    "An excerpt is provided below if you would like to understand the dynamics of the system:\n",
    "\n",
    "> ##### A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Run the following code to visualize the system dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "env.reset()\n",
    "\n",
    "num_steps_to_viz = 0\n",
    "for i in range(num_steps_to_viz):\n",
    "   plt.imshow(env.render())\n",
    "   display.display(plt.gcf())    \n",
    "   display.clear_output(wait=True)\n",
    "   env.step(random.randrange(0,2)) # take a random action\n",
    "\n",
    "env.close()\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the states represent (from the environment) and what the actions do (given to the environment)\n",
    "```   \n",
    "    Observation: \n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        ------------------------------------------------------\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24°           24°\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "        \n",
    "    Action:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        -------------------------------\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "```\n",
    "These states are returned as some weird datatypes, but a little modification can easily turn them into tensors *(you will find this a common issue, turning things to/from tensors)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create the Deep Q-Learning Network (DQN) that will play cartpole for us. If you remember from the lesson, Q-learning is powerful because it allows for \"experience replay\" where transitions can be saved inside of a memory buffer and then \"re-experienced\" by the model to learn. Note that technically this is not the Q-function as it only operates on the state and outputes an action. **This is more analogous to a raw policy.**\n",
    "\n",
    "Take a look at your older projects for a refresher on how to make a neural network. The specification for the network itself is detailed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a DQN \n",
    "## TODO: Create a neural network called DQN that has variable input and output layer sizes (input variable should be state_dim, output is action_dim). \n",
    "## TODO: It should have 2+ layers with relu activations for all but the last layer, which should use an activation function that results in a probability distribution\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Hidden layers don't always have to be smaller than the layer before it! Especially if the input dimensionality is small\n",
    "- Remind yourself what activations are and why they are useful. Which ones can result in a probability distribution?\n",
    "- Make sure to name your class \"DQN\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#! Answer\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.l1 = nn.Linear(state_dim, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, action_dim)\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.relu(self.l1(input))\n",
    "        x = self.relu(self.l2(x))\n",
    "        output = self.softmax(self.l3(x))\n",
    "        return output\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have defined a \"named tuple\" that will store a state \"transition\". Essentially it will hold a previous state (`state_a`) an action taken on that state (`action`), the resultant state from taking that state-action pair (`state_b`) and the reward for taking that state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Creates a named tuple that we can add to\n",
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    ('state_a', 'action', 'state_b', 'reward')\n",
    ")\n",
    "\n",
    "# Example of creating a named tuple\n",
    "\n",
    "t = Transition([0,0,0,0], 1, [1,1], 0.5)\n",
    "\n",
    "# You can check the value of, say, \"action\" by printing t.action\n",
    "\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be implementing a memory storage class that will be accessed in the training loop to \"replay\" memories to make the model better. What is nice about Q-learning is that \"optimization is almost always performed off-policy, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained.\" ([Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)). You can read more about Replay buffers [here](https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits)! Here are some important excerpts:\n",
    ">More efficient use of previous experience, by learning with it multiple times. This is key when gaining real-world experience is costly, you can get full use of it. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair.\n",
    "\n",
    ">Better convergence behaviour when training a function approximator. Partly this is because the data is more like i.i.d. data assumed in most supervised learning convergence proofs. \n",
    "\n",
    "Implement the following class. The spec has been written for you. You can read up on the deque class [here](https://docs.python.org/3/library/collections.html#collections.deque). It is the data structure we use to store Transition tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a memory buffer storage object\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Read up on how to use the deque object\n",
    "- The random.sample() method may help you here\n",
    "\"\"\"\n",
    "\n",
    "class TransitionMemoryStorage():\n",
    "    \"\"\"A class to hold a buffer of transition tuples that can be sampled from to run experience replay on a DQN agent\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Creates a buffer to hold transition tuples\n",
    "\n",
    "        Args:\n",
    "            capacity (int): How many elements the buffer can hold at a time\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_transition(self, t):\n",
    "        \"\"\"Adds a transition to the buffer\n",
    "\n",
    "        Args:\n",
    "            t (tuple): A Transition tuple\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"Selects num_samples unique samples from the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Number of samples to pull\n",
    "\n",
    "        Returns:\n",
    "            list: Sample list of transitions from the buffer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def can_sample(self, num_samples):\n",
    "        \"\"\"Checks if there are at least num_samples samples in the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): How many samples to check validity for\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the TransitionMemoryStorage object can be sampled on\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "#! Answer\n",
    "class TransitionMemoryStorage():\n",
    "    \"\"\"A class to hold a buffer of transition tuples that can be sampled from to run experience replay on a DQN agent\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Creates a buffer to hold transition tuples\n",
    "\n",
    "        Args:\n",
    "            capacity (int): How many elements the buffer can hold at a time\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_transition(self, t):\n",
    "        \"\"\"Adds a transition to the buffer\n",
    "\n",
    "        Args:\n",
    "            t (tuple): A Transition tuple\n",
    "        \"\"\"\n",
    "        self.buffer.append(t)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"Selects num_samples unique samples from the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Number of samples to pull\n",
    "\n",
    "        Returns:\n",
    "            list: Sample list of transitions from the buffer\n",
    "        \"\"\"\n",
    "        return random.sample(list(self.buffer), num_samples)\n",
    "\n",
    "    def can_sample(self, num_samples):\n",
    "        \"\"\"Checks if there are at least num_samples samples in the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): How many samples to check validity for\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the TransitionMemoryStorage object can be sampled on\n",
    "        \"\"\"\n",
    "        return len(self.buffer) >= num_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement an explore/exploit strategy called \"epsilon-greediness\". This was explained in the megadoc but if you need a refresher check out this link [here](https://www.baeldung.com/cs/epsilon-greedy-q-learning#:~:text=The%20epsilon%2Dgreedy%20approach%20selects,what%20we%20have%20already%20learned.) and read section 5.2. We will use this class when deciding wether to explore (take a random action) or exploit (use the model and hope it knows what it's doign by then)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Look up ways the epsilon-greedy equation has been implemented and see which one works best here\n",
    "- The `should_explore` method should have an <epsilon> probability of returning True where <epsilon> is the epsilon value at `current_step`\n",
    "- Use random.random()\n",
    "\"\"\"\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    \"\"\"Strategy for use in an agent, implements exponential decay epsilon greedy\n",
    "    \"\"\"\n",
    "    def __init__(self, max_epsilon, min_epsilon, decay):\n",
    "        \"\"\"Initializes an Epsilon Greedy strategy\n",
    "\n",
    "        Args:\n",
    "            max_epsilon (float): The initial epsilon value\n",
    "            min_epsilon (float): The ending epsilon value\n",
    "            decay (float): The rate at which the epsilon value will decay\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def should_explore(self, current_step):\n",
    "        \"\"\"Returns True if, according to this strategy at the current timestep, the agent should explore and False otherwise\n",
    "\n",
    "        Args:\n",
    "            current_step (int): How many steps the agent has taken (persists through episodes and failures)\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the agent should explore (take a random action)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _get_explore_prob(self, current_step):\n",
    "        \"\"\"Returns the epsilon value at a certain timestep\n",
    "\n",
    "        Args:\n",
    "            current_step (int): The number of steps that the agent has taken\n",
    "\n",
    "        Returns:\n",
    "            float: Epsilon value at this current timestep\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "#! Answer\n",
    "class EpsilonGreedyStrategy():\n",
    "    \"\"\"Strategy for use in an agent, implements exponential decay epsilon greedy\n",
    "    \"\"\"\n",
    "    def __init__(self, max_epsilon, min_epsilon, decay):\n",
    "        \"\"\"Initializes an Epsilon Greedy strategy\n",
    "\n",
    "        Args:\n",
    "            max_epsilon (float): The initial epsilon value\n",
    "            min_epsilon (float): The ending epsilon value\n",
    "            decay (float): The rate at which the epsilon value will decay\n",
    "        \"\"\"\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay = decay\n",
    "\n",
    "    def should_explore(self, current_step):\n",
    "        \"\"\"Returns True if, according to this strategy, the agent should explore and false otherwise\n",
    "\n",
    "        Args:\n",
    "            current_step (int): How many steps the agent has taken (persists through episodes and failures)\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the agent should explore (take a random action)\n",
    "        \"\"\"\n",
    "        if(random.random() < self._get_explore_prob(current_step)):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _get_explore_prob(self, current_step):\n",
    "        \"\"\"Returns the epsilon value at a certain timestep\n",
    "\n",
    "        Args:\n",
    "            current_step (int): The number of steps that the agent has taken\n",
    "\n",
    "        Returns:\n",
    "            float: Epsilon value at this current timestep\n",
    "        \"\"\"\n",
    "        return self.min_epsilon + (self.max_epsilon - self.min_epsilon) * math.exp(-1. * current_step * self.decay)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Agent tht will actually interact with the environment. Read the tips carefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Create an Agent class that can navigate the environment\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Be sure to increment the step counter whenever an action is selected\n",
    "- If the strategy says to explore, select a random action. Otherwise, use the model!\n",
    "- The model gives you probabilities (i.e. [0.74, 0.26]) saying which action it thinks is better. Use this information to choose an action.\n",
    "- Scroll up to remind yourself what integers are valid actions (and therefore return values for `select_action`)\n",
    "\"\"\"\n",
    "class Agent():\n",
    "    \"\"\"Agent that acts within the environment using a dqn policy\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy):\n",
    "        \"\"\"Initializes the agent with a strategy for explore vs. exploit\n",
    "\n",
    "        Args:\n",
    "            strategy: Strategy object that dictates to explore or exploit\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.current_step = 0\n",
    "\n",
    "    def select_action(self, input_state, policy_dqn):\n",
    "        \"\"\"Selects an action based on a state and a policy network\n",
    "\n",
    "        Args:\n",
    "            input_state: The state to select an action based on\n",
    "            policy_dqn: Policy network that outputs probabilities to take actions within the action space\n",
    "\n",
    "        Returns:\n",
    "            int: Action to take based on state\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "#! Answer         \n",
    "class Agent():\n",
    "    \"\"\"Agent that acts within the environment using a dqn policy\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy):\n",
    "        \"\"\"Initializes the agent with a strategy for explore vs. exploit\n",
    "\n",
    "        Args:\n",
    "            strategy: Strategy object that dictates to explore or exploit\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.current_step = 0\n",
    "\n",
    "    def select_action(self, input_state, policy_dqn):\n",
    "        \"\"\"Selects an action based on a state and a policy network\n",
    "\n",
    "        Args:\n",
    "            input_state: The state to select an action based on\n",
    "            policy_dqn: Policy network that outputs probabilities to take actions within the action space\n",
    "\n",
    "        Returns:\n",
    "            int: Action to take based on state\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        if(self.strategy.should_explore(self.current_step)):\n",
    "            # Explore\n",
    "            return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
    "        else:\n",
    "            # Exploit\n",
    "            with torch.no_grad():\n",
    "                optimal_action = policy_dqn(input_state).max(1)[1].view(1, 1)\n",
    "                return optimal_action\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! You're almost done with all the setup. Below are some small helper methods that will help you convert between different datatypes we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "def t(np_arr):\n",
    "    \"\"\"Converts a numpy array to a tensor\n",
    "\n",
    "    Args:\n",
    "        numpy_array: Numpy array to convert\n",
    "\n",
    "    Returns:\n",
    "        Tensor with the data from the numpy array\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(np_arr)\n",
    "\n",
    "def t_int(num):\n",
    "    \"\"\"Converts a single integer into a torch tensor\n",
    "\n",
    "    Args:\n",
    "        num (int): The integer to wrap\n",
    "\n",
    "    Returns:\n",
    "        Tensor with the data from the integer\n",
    "    \"\"\"\n",
    "    return torch.tensor([num])\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to the training loop. This is hard and requires some specific lines of code, and i made it myself so there is no tutorial online. However, the answer key does exist if you get stuck for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnum_episodes = 1000\\nfor episode in range(num_episodes):\\n    #print(f\"Episode {episode}\")\\n    current_state, _ = env.reset()\\n    done = False\\n\\n    while not (done):\\n        action = agent.select_action(t(current_state), policy_net)\\n        next_state, reward, terminated, truncated, _ = env.step(action)\\n        done = terminated or truncated\\n\\n        if (done):\\n            break\\n\\n        memory.add_transition(Transition(current_state, action, next_state, reward))\\n        optimize_model()\\n        current_state = next_state\\n        \\n    target_net_state_dict = target_net.state_dict()\\n    policy_net_state_dict = policy_net.state_dict()\\n    for key in policy_net_state_dict:\\n        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\\n    target_net.load_state_dict(target_net_state_dict)\\n'"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import count\n",
    "#! Answer\n",
    "e_greedy_strategy = EpsilonGreedyStrategy(max_epsilon=0.9, min_epsilon=0.05, decay=0.99)\n",
    "agent = Agent(strategy=e_greedy_strategy)\n",
    "memory = TransitionMemoryStorage(1000)\n",
    "\n",
    "policy_net = DQN(state_dim, action_dim)\n",
    "target_net = DQN(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "target_update = 0\n",
    "\n",
    "\"\"\"\n",
    "def optimize_model():\n",
    "    if(memory.can_sample(batch_size)):\n",
    "        experiences = memory.sample(batch_size)\n",
    "\n",
    "        for transition in experiences:\n",
    "            current_q = policy_net(t(transition.state_a))[transition.action]\n",
    "            next_q = torch.max(target_net(t(transition.state_b)))\n",
    "\n",
    "            target_q_values = transition.reward + (GAMMA * next_q)\n",
    "\n",
    "            loss = loss_func(current_q, target_q_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\"\"\"\n",
    "\n",
    "def optimize_model():\n",
    "    if not memory.can_sample(BATCH_SIZE):\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.state_b)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.state_b if s is not None])\n",
    "    state_batch = torch.cat(batch.state_a)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "num_episodes = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.add_transition(Transition(state, action, next_state, reward))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "\"\"\"\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    #print(f\"Episode {episode}\")\n",
    "    current_state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not (done):\n",
    "        action = agent.select_action(t(current_state), policy_net)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if (done):\n",
    "            break\n",
    "\n",
    "        memory.add_transition(Transition(current_state, action, next_state, reward))\n",
    "        optimize_model()\n",
    "        current_state = next_state\n",
    "        \n",
    "    target_net_state_dict = target_net.state_dict()\n",
    "    policy_net_state_dict = policy_net.state_dict()\n",
    "    for key in policy_net_state_dict:\n",
    "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "    target_net.load_state_dict(target_net_state_dict)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "done = False\n",
    "this_run = 0\n",
    "for t in count():\n",
    "    action = agent.select_action(state, policy_net)\n",
    "    state,reward,terminated,truncated,_ = env.step(action.item())\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    done = terminated or truncated\n",
    "    this_run+=1\n",
    "    longest_run = max(this_run,longest_run) \n",
    "    if done:\n",
    "        print(this_run)\n",
    "        break\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
