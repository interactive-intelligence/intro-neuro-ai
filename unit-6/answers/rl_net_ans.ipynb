{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Project! **[ANSWER SHEET]**\n",
    "\n",
    "##### How to work through this project:\n",
    "- Go cell by cell and finish the marked #TODO's\n",
    "- You don't need to touch the code marked between the `#---------#`. Those are puzzle pieces that your code will fit into!\n",
    "    - However, I **STRONGLY** encourage you to understand every single line between those blocks. They are essential!\n",
    "    - It is crucial that your variable names are what we expect them to be, or the puzzle pieces won't fit.\n",
    "- Tutorials/helpful information will be placed in the `.md` cells above the \"work\" cells. Consult them if you are stuck.\n",
    "- If you REALLY cannot find the correct code to make the cell run, consult the `[proj]-ans.ipynb`.\n",
    "- The final product (what we expect to see if you run all the cells consecutively) will be placed in the `answers/` directory.\n",
    "    - Chances are your output won't be the exact same (stochasticity!) but it should be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get used to these imports!\n",
    "#----------------------------------------------------------------#\n",
    "#To install: pip install numpy\n",
    "import numpy as np \n",
    "#To install: pip install matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "#To install: pip install torch (not GPU compatible)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#To install: pip install gymnasium[classic_control]\n",
    "import gymnasium as gym\n",
    "\n",
    "# No pip install needed\n",
    "from IPython import display\n",
    "import random\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be implementing DQN for Cartpole, which is a common RL benchmark from OpenAI! First, lets visualize the environment that our Deep Q-Learnign Network will operate in. You can read more about cartpole [here](https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288)\n",
    "\n",
    "An excerpt is provided below if you would like to understand the dynamics of the system:\n",
    "\n",
    "> ##### A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Run the following code to visualize the system dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "env.reset()\n",
    "\n",
    "num_steps_to_viz = 0\n",
    "for i in range(num_steps_to_viz):\n",
    "   plt.imshow(env.render())\n",
    "   display.display(plt.gcf())    \n",
    "   display.clear_output(wait=True)\n",
    "   env.step(random.randrange(0,2)) # take a random action\n",
    "\n",
    "env.close()\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the states represent (from the environment) and what the actions do (given to the environment)\n",
    "```   \n",
    "    Observation: \n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        ------------------------------------------------------\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24°           24°\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "        \n",
    "    Action:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        -------------------------------\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "```\n",
    "These states are returned as some weird datatypes, but a little modification can easily turn them into tensors *(you will find this a common issue, turning things to/from tensors)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create the Deep Q-Learning Network (DQN) that will play cartpole for us. If you remember from the lesson, Q-learning is powerful because it allows for \"experience replay\" where transitions can be saved inside of a memory buffer and then \"re-experienced\" by the model to learn. Note that technically this is not the Q-function as it only operates on the state and outputes an action. **This is more analogous to a raw policy.**\n",
    "\n",
    "Take a look at your older projects for a refresher on how to make a neural network. The specification for the network itself is detailed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a DQN \n",
    "## TODO: Create a neural network called DQN that has variable input and output layer sizes (input variable should be state_dim, output is action_dim). \n",
    "## TODO: It should have 2+ layers with relu activations for all but the last layer, which should use an activation function that results in a probability distribution\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Hidden layers don't always have to be smaller than the layer before it! Especially if the input dimensionality is small\n",
    "- Remind yourself what activations are and why they are useful. Which ones can result in a probability distribution?\n",
    "- Make sure to name your class \"DQN\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#! Answer\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.l1 = nn.Linear(state_dim, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, action_dim)\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.relu(self.l1(input))\n",
    "        x = self.relu(self.l2(x))\n",
    "        output = self.softmax(self.l3(x))\n",
    "        return output\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have defined a \"named tuple\" that will store a state \"transition\". Essentially it will hold a previous state (`state_a`) an action taken on that state (`action`), the resultant state from taking that state-action pair (`state_b`) and the reward for taking that state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Creates a named tuple that we can add to\n",
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    ('state_a', 'action', 'state_b', 'reward')\n",
    ")\n",
    "\n",
    "# Example of creating a named tuple\n",
    "\n",
    "t = Transition([0,0,0,0], 1, [1,1], 0.5)\n",
    "\n",
    "# You can check the value of, say, \"action\" by printing t.action\n",
    "\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be implementing a memory storage class that will be accessed in the training loop to \"replay\" memories to make the model better. What is nice about Q-learning is that \"optimization is almost always performed off-policy, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained.\" ([Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)). You can read more about Replay buffers [here](https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits)! Here are some important excerpts:\n",
    ">More efficient use of previous experience, by learning with it multiple times. This is key when gaining real-world experience is costly, you can get full use of it. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair.\n",
    "\n",
    ">Better convergence behaviour when training a function approximator. Partly this is because the data is more like i.i.d. data assumed in most supervised learning convergence proofs. \n",
    "\n",
    "Implement the following class. The spec has been written for you. You can read up on the deque class [here](https://docs.python.org/3/library/collections.html#collections.deque). It is the data structure we use to store Transition tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a memory buffer storage object\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Read up on how to use the deque object\n",
    "- The random.sample() method may help you here\n",
    "\"\"\"\n",
    "\n",
    "class TransitionMemoryStorage():\n",
    "    \"\"\"A class to hold a buffer of transition tuples that can be sampled from to run experience replay on a DQN agent\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Creates a buffer to hold transition tuples\n",
    "\n",
    "        Args:\n",
    "            capacity (int): How many elements the buffer can hold at a time\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_transition(self, t):\n",
    "        \"\"\"Adds a transition to the buffer\n",
    "\n",
    "        Args:\n",
    "            t (tuple): A Transition tuple\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"Selects num_samples unique samples from the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Number of samples to pull\n",
    "\n",
    "        Returns:\n",
    "            list: Sample list of transitions from the buffer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def can_sample(self, num_samples):\n",
    "        \"\"\"Checks if there are at least num_samples samples in the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): How many samples to check validity for\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the TransitionMemoryStorage object can be sampled on\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "#! Answer\n",
    "class TransitionMemoryStorage():\n",
    "    \"\"\"A class to hold a buffer of transition tuples that can be sampled from to run experience replay on a DQN agent\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Creates a buffer to hold transition tuples\n",
    "\n",
    "        Args:\n",
    "            capacity (int): How many elements the buffer can hold at a time\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_transition(self, t):\n",
    "        \"\"\"Adds a transition to the buffer\n",
    "\n",
    "        Args:\n",
    "            t (tuple): A Transition tuple\n",
    "        \"\"\"\n",
    "        self.buffer.append(t)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"Selects num_samples unique samples from the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Number of samples to pull\n",
    "\n",
    "        Returns:\n",
    "            list: Sample list of transitions from the buffer\n",
    "        \"\"\"\n",
    "        return random.sample(list(self.buffer), num_samples)\n",
    "\n",
    "    def can_sample(self, num_samples):\n",
    "        \"\"\"Checks if there are at least num_samples samples in the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): How many samples to check validity for\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the TransitionMemoryStorage object can be sampled on\n",
    "        \"\"\"\n",
    "        return len(self.buffer) >= num_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement an explore/exploit strategy called \"epsilon-greediness\". This was explained in the megadoc but if you need a refresher check out this link [here](https://www.baeldung.com/cs/epsilon-greedy-q-learning#:~:text=The%20epsilon%2Dgreedy%20approach%20selects,what%20we%20have%20already%20learned.) and read section 5.2. We will use this class when deciding wether to explore (take a random action) or exploit (use the model and hope it knows what it's doign by then)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Look up ways the epsilon-greedy equation has been implemented and see which one works best here\n",
    "- The `should_explore` method should have an <epsilon> probability of returning True where <epsilon> is the epsilon value at `current_step`\n",
    "- Use random.random()\n",
    "\"\"\"\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    \"\"\"Strategy for use in an agent, implements exponential decay epsilon greedy\n",
    "    \"\"\"\n",
    "    def __init__(self, max_epsilon, min_epsilon, decay):\n",
    "        \"\"\"Initializes an Epsilon Greedy strategy\n",
    "\n",
    "        Args:\n",
    "            max_epsilon (float): The initial epsilon value\n",
    "            min_epsilon (float): The ending epsilon value\n",
    "            decay (float): The rate at which the epsilon value will decay\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def should_explore(self, current_step):\n",
    "        \"\"\"Returns True if, according to this strategy at the current timestep, the agent should explore and False otherwise\n",
    "\n",
    "        Args:\n",
    "            current_step (int): How many steps the agent has taken (persists through episodes and failures)\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the agent should explore (take a random action)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _get_explore_prob(self, current_step):\n",
    "        \"\"\"Returns the epsilon value at a certain timestep\n",
    "\n",
    "        Args:\n",
    "            current_step (int): The number of steps that the agent has taken\n",
    "\n",
    "        Returns:\n",
    "            float: Epsilon value at this current timestep\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "#! Answer\n",
    "class EpsilonGreedyStrategy():\n",
    "    \"\"\"Strategy for use in an agent, implements exponential decay epsilon greedy\n",
    "    \"\"\"\n",
    "    def __init__(self, max_epsilon, min_epsilon, decay):\n",
    "        \"\"\"Initializes an Epsilon Greedy strategy\n",
    "\n",
    "        Args:\n",
    "            max_epsilon (float): The initial epsilon value\n",
    "            min_epsilon (float): The ending epsilon value\n",
    "            decay (float): The rate at which the epsilon value will decay\n",
    "        \"\"\"\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay = decay\n",
    "\n",
    "    def should_explore(self, current_step):\n",
    "        \"\"\"Returns True if, according to this strategy, the agent should explore and false otherwise\n",
    "\n",
    "        Args:\n",
    "            current_step (int): How many steps the agent has taken (persists through episodes and failures)\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the agent should explore (take a random action)\n",
    "        \"\"\"\n",
    "        if(random.random() < self._get_explore_prob(current_step)):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _get_explore_prob(self, current_step):\n",
    "        \"\"\"Returns the epsilon value at a certain timestep\n",
    "\n",
    "        Args:\n",
    "            current_step (int): The number of steps that the agent has taken\n",
    "\n",
    "        Returns:\n",
    "            float: Epsilon value at this current timestep\n",
    "        \"\"\"\n",
    "        return self.min_epsilon + (self.max_epsilon - self.min_epsilon) * math.exp(-1. * current_step * self.decay)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Agent tht will actually interact with the environment. Read the tips carefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Create an Agent class that can navigate the environment\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Be sure to increment the step counter whenever an action is selected\n",
    "- If the strategy says to explore, select a random action. Otherwise, use the model!\n",
    "- The model gives you probabilities (i.e. [0.74, 0.26]) saying which action it thinks is better. Use this information to choose an action.\n",
    "- Scroll up to remind yourself what integers are valid actions (and therefore return values for `select_action`)\n",
    "\"\"\"\n",
    "class Agent():\n",
    "    \"\"\"Agent that acts within the environment using a dqn policy\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy):\n",
    "        \"\"\"Initializes the agent with a strategy for explore vs. exploit\n",
    "\n",
    "        Args:\n",
    "            strategy: Strategy object that dictates to explore or exploit\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.current_step = 0\n",
    "\n",
    "    def select_action(self, input_state, policy_dqn):\n",
    "        \"\"\"Selects an action based on a state and a policy network\n",
    "\n",
    "        Args:\n",
    "            input_state: The state to select an action based on\n",
    "            policy_dqn: Policy network that outputs probabilities to take actions within the action space\n",
    "\n",
    "        Returns:\n",
    "            int: Action to take based on state\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "#! Answer         \n",
    "class Agent():\n",
    "    \"\"\"Agent that acts within the environment using a dqn policy\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy):\n",
    "        \"\"\"Initializes the agent with a strategy for explore vs. exploit\n",
    "\n",
    "        Args:\n",
    "            strategy: Strategy object that dictates to explore or exploit\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.current_step = 0\n",
    "\n",
    "    def select_action(self, input_state, policy_dqn):\n",
    "        \"\"\"Selects an action based on a state and a policy network\n",
    "\n",
    "        Args:\n",
    "            input_state: The state to select an action based on\n",
    "            policy_dqn: Policy network that outputs probabilities to take actions within the action space\n",
    "\n",
    "        Returns:\n",
    "            int: Action to take based on state\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        if(self.strategy.should_explore(self.current_step)):\n",
    "            # Explore\n",
    "            return random.randint(0, 1)\n",
    "        else:\n",
    "            # Exploit\n",
    "            with torch.no_grad():\n",
    "                optimal_action = torch.argmax(policy_dqn(input_state))\n",
    "                return optimal_action.item()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! You're almost done with all the setup. Below are some small helper methods that will help you convert between different datatypes we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "def t(np_arr):\n",
    "    \"\"\"Converts a numpy array to a tensor\n",
    "\n",
    "    Args:\n",
    "        numpy_array: Numpy array to convert\n",
    "\n",
    "    Returns:\n",
    "        Tensor with the data from the numpy array\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(np_arr)\n",
    "\n",
    "def t_num(num):\n",
    "    \"\"\"Converts a single integer into a torch tensor\n",
    "\n",
    "    Args:\n",
    "        num (int/float): The integer to wrap\n",
    "\n",
    "    Returns:\n",
    "        Tensor with the data from the integer\n",
    "    \"\"\"\n",
    "    return torch.tensor([num])\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to the training loop. This is hard and requires some specific lines of code, and i made it myself so there is no tutorial online. However, the answer key does exist if you get stuck for too long.\n",
    "\n",
    "We use two networks, the policy and the target network, with a slow update on the target network to match the policy one. This \"lagging self-tuning\" ensures the model wont be caught in a self-optimization loop. After all, DQN is based on optimizing the Bellman equation, not necessarily towards maximizing reward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnum_episodes = 1000\\nfor episode in range(num_episodes):\\n    #print(f\"Episode {episode}\")\\n    current_state, _ = env.reset()\\n    done = False\\n\\n    while not (done):\\n        action = agent.select_action(t(current_state), policy_net)\\n        next_state, reward, terminated, truncated, _ = env.step(action)\\n        done = terminated or truncated\\n\\n        if (done):\\n            break\\n\\n        memory.add_transition(Transition(current_state, action, next_state, reward))\\n        optimize_model()\\n        current_state = next_state\\n        \\n    target_net_state_dict = target_net.state_dict()\\n    policy_net_state_dict = policy_net.state_dict()\\n    for key in policy_net_state_dict:\\n        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\\n    target_net.load_state_dict(target_net_state_dict)\\n'"
      ]
     },
     "execution_count": 1222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import count\n",
    "#! Answer\n",
    "e_greedy_strategy = EpsilonGreedyStrategy(max_epsilon=0.9, min_epsilon=0.05, decay=0.99)\n",
    "agent = Agent(strategy=e_greedy_strategy)\n",
    "memory = TransitionMemoryStorage(1000)\n",
    "\n",
    "policy_net = DQN(state_dim, action_dim)\n",
    "target_net = DQN(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "target_update = 0\n",
    "\n",
    "\"\"\"\n",
    "def optimize_model():\n",
    "    if(memory.can_sample(batch_size)):\n",
    "        experiences = memory.sample(batch_size)\n",
    "\n",
    "        for transition in experiences:\n",
    "            current_q = policy_net(t(transition.state_a))[transition.action]\n",
    "            next_q = torch.max(target_net(t(transition.state_b)))\n",
    "\n",
    "            target_q_values = transition.reward + (GAMMA * next_q)\n",
    "\n",
    "            loss = loss_func(current_q, target_q_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\"\"\"\n",
    "\n",
    "def optimize_model():\n",
    "    if not memory.can_sample(BATCH_SIZE):\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.state_b)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.state_b if s is not None])\n",
    "    state_batch = torch.cat(batch.state_a)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "num_episodes = 500\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        action = torch.tensor([action]).unsqueeze(1)\n",
    "        reward = torch.tensor([reward])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.add_transition(Transition(state, action, next_state, reward))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "\"\"\"\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    #print(f\"Episode {episode}\")\n",
    "    current_state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not (done):\n",
    "        action = agent.select_action(t(current_state), policy_net)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if (done):\n",
    "            break\n",
    "\n",
    "        memory.add_transition(Transition(current_state, action, next_state, reward))\n",
    "        optimize_model()\n",
    "        current_state = next_state\n",
    "        \n",
    "    target_net_state_dict = target_net.state_dict()\n",
    "    policy_net_state_dict = policy_net.state_dict()\n",
    "    for key in policy_net_state_dict:\n",
    "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "    target_net.load_state_dict(target_net_state_dict)\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to test your model! The following method runs a few episodes and tells you the highest # of steps the agent was able to stay alive for across those episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest reward across 100 trials: 343\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------#\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "num_trials = 100\n",
    "overall_max_reward = 0\n",
    "\n",
    "done = False\n",
    "for i in range(10):\n",
    "    episode_reward = 0\n",
    "    for t in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        state,reward,terminated,truncated,_ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_reward+=1\n",
    "        overall_max_reward= max(episode_reward,overall_max_reward) \n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(f\"Largest reward across {num_trials} trials: {overall_max_reward}\")\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method visualizes one epsiode so you can see it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1224], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n\u001b[0;32m      5\u001b[0m     plt\u001b[39m.\u001b[39mimshow(env\u001b[39m.\u001b[39mrender())\n\u001b[1;32m----> 6\u001b[0m     display\u001b[39m.\u001b[39;49mdisplay(plt\u001b[39m.\u001b[39;49mgcf())    \n\u001b[0;32m      7\u001b[0m     display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action(state, policy_net)\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\IPython\\core\\formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    175\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39m(extras \u001b[39m+\u001b[39m args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\IPython\\core\\formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    222\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\IPython\\core\\formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    339\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    340\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mprint_figure(bytes_io, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\backend_bases.py:2314\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2308\u001b[0m     renderer \u001b[39m=\u001b[39m _get_renderer(\n\u001b[0;32m   2309\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure,\n\u001b[0;32m   2310\u001b[0m         functools\u001b[39m.\u001b[39mpartial(\n\u001b[0;32m   2311\u001b[0m             print_method, orientation\u001b[39m=\u001b[39morientation)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2313\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mgetattr\u001b[39m(renderer, \u001b[39m\"\u001b[39m\u001b[39m_draw_disabled\u001b[39m\u001b[39m\"\u001b[39m, nullcontext)():\n\u001b[1;32m-> 2314\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m   2316\u001b[0m \u001b[39mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2317\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[0;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\figure.py:3082\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3079\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3081\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3082\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3083\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3085\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[0;32m   3086\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\axes\\_base.py:3100\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3097\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m   3098\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[1;32m-> 3100\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3101\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3103\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   3104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\image.py:641\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[0;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 641\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[0;32m    642\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[0;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\image.py:949\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[1;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[0;32m    946\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[0;32m    947\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[0;32m    948\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[1;32m--> 949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[0;32m    950\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\image.py:553\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[1;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[0;32m    551\u001b[0m     A \u001b[39m=\u001b[39m _rgb_to_rgba(A)\n\u001b[0;32m    552\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_scalar_alpha()\n\u001b[1;32m--> 553\u001b[0m output_alpha \u001b[39m=\u001b[39m _resample(  \u001b[39m# resample alpha channel\u001b[39;49;00m\n\u001b[0;32m    554\u001b[0m     \u001b[39mself\u001b[39;49m, A[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m3\u001b[39;49m], out_shape, t, alpha\u001b[39m=\u001b[39;49malpha)\n\u001b[0;32m    555\u001b[0m output \u001b[39m=\u001b[39m _resample(  \u001b[39m# resample rgb channels\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[39mself\u001b[39m, _rgb_to_rgba(A[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m3\u001b[39m]), out_shape, t, alpha\u001b[39m=\u001b[39malpha)\n\u001b[0;32m    557\u001b[0m output[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m output_alpha  \u001b[39m# recombine rgb and alpha\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\varun\\anaconda3\\envs\\intro-course\\lib\\site-packages\\matplotlib\\image.py:207\u001b[0m, in \u001b[0;36m_resample\u001b[1;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m resample \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     resample \u001b[39m=\u001b[39m image_obj\u001b[39m.\u001b[39mget_resample()\n\u001b[1;32m--> 207\u001b[0m _image\u001b[39m.\u001b[39;49mresample(data, out, transform,\n\u001b[0;32m    208\u001b[0m                 _interpd_[interpolation],\n\u001b[0;32m    209\u001b[0m                 resample,\n\u001b[0;32m    210\u001b[0m                 alpha,\n\u001b[0;32m    211\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filternorm(),\n\u001b[0;32m    212\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filterrad())\n\u001b[0;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoCUlEQVR4nO3df3RU9Z3/8dfkp4QwEwMkk5QEUSgQIdgGDLO2Li0pAaIrNZ6vWhZilwNHNvEUYimmS0XsHuPinvVHV+GP3Yp7jpSWfkUXKtgYJNQafpiS5ZdmhcNusGQSKpsZiBKSzOf7h1/udhSRhJD5zOT5OOd6MvfzuXfe93MS5uW993PHZYwxAgAAsEhcpAsAAAD4LAIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBORAPK888/rxtuuEHXXXedCgsLtW/fvkiWAwAALBGxgPLLX/5SlZWVWr16tf7whz9oypQpKi4uVltbW6RKAgAAlnBF6ssCCwsLNW3aNP3zP/+zJCkUCiknJ0cPPfSQHnnkkUiUBAAALJEQiTe9cOGCGhoaVFVV5ayLi4tTUVGR6uvrP9e/s7NTnZ2dzutQKKQzZ85o+PDhcrlcA1IzAAC4OsYYnT17VtnZ2YqLu/xFnIgElD/96U/q6elRZmZm2PrMzEy9//77n+tfXV2tNWvWDFR5AADgGjp58qRGjRp12T4RCSi9VVVVpcrKSud1IBBQbm6uTp48KbfbHcHKAADAlQoGg8rJydGwYcO+tG9EAsqIESMUHx+v1tbWsPWtra3yer2f65+cnKzk5OTPrXe73QQUAACizJXcnhGRWTxJSUkqKChQbW2tsy4UCqm2tlY+ny8SJQEAAItE7BJPZWWlysrKNHXqVN1666165pln1NHRoe9///uRKgkAAFgiYgHl3nvv1enTp/Xoo4/K7/frlltu0Y4dOz534ywAABh8IvYclKsRDAbl8XgUCAS4BwUAgCjRm89vvosHAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6/R5QHnvsMblcrrBlwoQJTvv58+dVXl6u4cOHKzU1VaWlpWptbe3vMgAAQBS7JmdQbr75ZrW0tDjL22+/7bQtX75cW7du1ebNm1VXV6dTp07p7rvvvhZlAACAKJVwTXaakCCv1/u59YFAQP/6r/+qjRs36tvf/rYk6cUXX9TEiRO1Z88eTZ8+/VqUAwAAosw1OYPywQcfKDs7WzfeeKPmz5+v5uZmSVJDQ4O6urpUVFTk9J0wYYJyc3NVX1//hfvr7OxUMBgMWwAAQOzq94BSWFioDRs2aMeOHVq3bp1OnDihb37zmzp79qz8fr+SkpKUlpYWtk1mZqb8fv8X7rO6uloej8dZcnJy+rtsAABgkX6/xDNnzhzn5/z8fBUWFmr06NH61a9+pSFDhvRpn1VVVaqsrHReB4NBQgoAADHsmk8zTktL01e/+lUdO3ZMXq9XFy5cUHt7e1if1tbWS96zclFycrLcbnfYAgAAYtc1Dyjnzp3T8ePHlZWVpYKCAiUmJqq2ttZpb2pqUnNzs3w+37UuBQAARIl+v8Tzwx/+UHfeeadGjx6tU6dOafXq1YqPj9f9998vj8ejRYsWqbKyUunp6XK73XrooYfk8/mYwQMAABz9HlA+/PBD3X///froo480cuRIfeMb39CePXs0cuRISdLTTz+tuLg4lZaWqrOzU8XFxXrhhRf6uwwAABDFXMYYE+kieisYDMrj8SgQCHA/CgAAUaI3n998Fw8AALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDq9Dii7d+/WnXfeqezsbLlcLr366qth7cYYPfroo8rKytKQIUNUVFSkDz74IKzPmTNnNH/+fLndbqWlpWnRokU6d+7cVR0IAACIHb0OKB0dHZoyZYqef/75S7avXbtWzz33nNavX6+9e/dq6NChKi4u1vnz550+8+fP15EjR1RTU6Nt27Zp9+7dWrJkSd+PAgAAxBSXMcb0eWOXS1u2bNG8efMkfXr2JDs7Ww8//LB++MMfSpICgYAyMzO1YcMG3XfffXrvvfeUl5en/fv3a+rUqZKkHTt2aO7cufrwww+VnZ39pe8bDAbl8XgUCATkdrv7Wj4AABhAvfn87td7UE6cOCG/36+ioiJnncfjUWFhoerr6yVJ9fX1SktLc8KJJBUVFSkuLk579+695H47OzsVDAbDFgAAELv6NaD4/X5JUmZmZtj6zMxMp83v9ysjIyOsPSEhQenp6U6fz6qurpbH43GWnJyc/iwbAABYJipm8VRVVSkQCDjLyZMnI10SAAC4hvo1oHi9XklSa2tr2PrW1lanzev1qq2tLay9u7tbZ86ccfp8VnJystxud9gCAABiV78GlDFjxsjr9aq2ttZZFwwGtXfvXvl8PkmSz+dTe3u7GhoanD47d+5UKBRSYWFhf5YDAACiVEJvNzh37pyOHTvmvD5x4oQaGxuVnp6u3NxcLVu2TH//93+vcePGacyYMfrJT36i7OxsZ6bPxIkTNXv2bC1evFjr169XV1eXKioqdN99913RDB4AABD7eh1Q3n33XX3rW99yXldWVkqSysrKtGHDBv3oRz9SR0eHlixZovb2dn3jG9/Qjh07dN111znbvPzyy6qoqNDMmTMVFxen0tJSPffcc/1wOAAAIBZc1XNQIoXnoAAAEH0i9hwUAACA/kBAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgnV4HlN27d+vOO+9Udna2XC6XXn311bD2Bx54QC6XK2yZPXt2WJ8zZ85o/vz5crvdSktL06JFi3Tu3LmrOhAAABA7eh1QOjo6NGXKFD3//PNf2Gf27NlqaWlxll/84hdh7fPnz9eRI0dUU1Ojbdu2affu3VqyZEnvqwcAADEpobcbzJkzR3PmzLlsn+TkZHm93ku2vffee9qxY4f279+vqVOnSpJ+9rOfae7cufrHf/xHZWdn97YkAAAQY67JPSi7du1SRkaGxo8fr6VLl+qjjz5y2urr65WWluaEE0kqKipSXFyc9u7de8n9dXZ2KhgMhi0AACB29XtAmT17tv7t3/5NtbW1+od/+AfV1dVpzpw56unpkST5/X5lZGSEbZOQkKD09HT5/f5L7rO6uloej8dZcnJy+rtsAABgkV5f4vky9913n/Pz5MmTlZ+fr5tuukm7du3SzJkz+7TPqqoqVVZWOq+DwSAhBQCAGHbNpxnfeOONGjFihI4dOyZJ8nq9amtrC+vT3d2tM2fOfOF9K8nJyXK73WELAACIXdc8oHz44Yf66KOPlJWVJUny+Xxqb29XQ0OD02fnzp0KhUIqLCy81uUAAIAo0OtLPOfOnXPOhkjSiRMn1NjYqPT0dKWnp2vNmjUqLS2V1+vV8ePH9aMf/Uhjx45VcXGxJGnixImaPXu2Fi9erPXr16urq0sVFRW67777mMEDAAAkSS5jjOnNBrt27dK3vvWtz60vKyvTunXrNG/ePB04cEDt7e3Kzs7WrFmz9NOf/lSZmZlO3zNnzqiiokJbt25VXFycSktL9dxzzyk1NfWKaggGg/J4PAoEAlzuAQAgSvTm87vXAcUGBBQAAKJPbz6/+S4eAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOr78sEAAG0of7X9XHfzp52T6Zk2fKMypvgCoCMBAIKACs1tF2QsEP37tsn+vHfF3GGLlcrgGqCsC1xiUeANEv+r7zFMCXIKAAiHpGoUiXAKCfEVAARD/j/AdAjCCgAIh+XOIBYg4BBUAMMJxAAWIMAQVA1DOcQQFiDgEFQAww4hQKEFsIKACiH2dQgJhDQAEQ9cgnQOwhoACIASFCChBjCCgAop/hHhQg1hBQAEQ9ogkQewgoAKIf13eAmENAARD9uMQDxBwCCoCox4PagNhDQAEQA0KcQAFiDAEFAABYh4ACIOoZ7kEBYk6vAkp1dbWmTZumYcOGKSMjQ/PmzVNTU1NYn/Pnz6u8vFzDhw9XamqqSktL1draGtanublZJSUlSklJUUZGhlasWKHu7u6rPxoAgxP3oAAxp1cBpa6uTuXl5dqzZ49qamrU1dWlWbNmqaOjw+mzfPlybd26VZs3b1ZdXZ1OnTqlu+++22nv6elRSUmJLly4oHfeeUcvvfSSNmzYoEcffbT/jgrA4GIM50+AGOMyV3H7++nTp5WRkaG6ujrdfvvtCgQCGjlypDZu3Kh77rlHkvT+++9r4sSJqq+v1/Tp07V9+3bdcccdOnXqlDIzMyVJ69ev18qVK3X69GklJSV96fsGg0F5PB4FAgG53e6+lg8gCjT95mkFP3zvsn2yvl6irK/NUXzCl//7ASByevP5fVX3oAQCAUlSenq6JKmhoUFdXV0qKipy+kyYMEG5ubmqr6+XJNXX12vy5MlOOJGk4uJiBYNBHTly5JLv09nZqWAwGLYAgMMYLvMAMabPASUUCmnZsmW67bbbNGnSJEmS3+9XUlKS0tLSwvpmZmbK7/c7ff48nFxsv9h2KdXV1fJ4PM6Sk5PT17IBAEAU6HNAKS8v1+HDh7Vp06b+rOeSqqqqFAgEnOXkyZPX/D0BRA9jQpEuAUA/S+jLRhUVFdq2bZt2796tUaNGOeu9Xq8uXLig9vb2sLMora2t8nq9Tp99+/aF7e/iLJ+LfT4rOTlZycnJfSkVwKDANGMg1vTqDIoxRhUVFdqyZYt27typMWPGhLUXFBQoMTFRtbW1zrqmpiY1NzfL5/NJknw+nw4dOqS2tjanT01Njdxut/Ly8q7mWAAMVmQTIOb06gxKeXm5Nm7cqNdee03Dhg1z7hnxeDwaMmSIPB6PFi1apMrKSqWnp8vtduuhhx6Sz+fT9OnTJUmzZs1SXl6eFixYoLVr18rv92vVqlUqLy/nLAmAPjHGEFKAGNOrgLJu3TpJ0owZM8LWv/jii3rggQckSU8//bTi4uJUWlqqzs5OFRcX64UXXnD6xsfHa9u2bVq6dKl8Pp+GDh2qsrIyPf7441d3JAAGMdIJEGuu6jkokcJzUIDB40qeg5I5uUhfmXqn4pOGDFBVAPpiwJ6DAgA24DmyQOwhoACIfjzqHog5BBQA0Y+bZIGYQ0ABEPU+vZWOhALEEgIKgBhAOAFiDQEFQPSLvsmIAL4EAQVADOASDxBrCCgAoh4nUIDYQ0ABEP1MiBMoQIwhoACIejwFBYg9BBQA0c84/wEQIwgoAGIA4QSINQQUANGPR90DMYeAAiDqGR51D8QcAgqAGMBzUIBYQ0ABEP14EAoQcwgoAKIed6AAsYeAAiD68W3GQMwhoACIAYQTINYQUABEPU6gALGHgAIg+nGTLBBzCCgArDZ0xA1yxSVcts8nZz5UT9f5AaoIwEAgoACw2pD0bLni4y/bpzN4WqGuzgGqCMBAIKAAsJvLFekKAEQAAQWA3QgowKBEQAFgNZcIKMBgREABYDeXSyKkAIMOAQWA1Vwu/pkCBiP+8gEAgHUIKADsxk2ywKBEQAFgNy7xAINSr/7yq6urNW3aNA0bNkwZGRmaN2+empqawvrMmDFDLpcrbHnwwQfD+jQ3N6ukpEQpKSnKyMjQihUr1N3dffVHAyDmuJjHAwxKl39+9GfU1dWpvLxc06ZNU3d3t3784x9r1qxZOnr0qIYOHer0W7x4sR5//HHndUpKivNzT0+PSkpK5PV69c4776ilpUULFy5UYmKinnjiiX44JAAxxeViEg8wCPUqoOzYsSPs9YYNG5SRkaGGhgbdfvvtzvqUlBR5vd5L7uO3v/2tjh49qjfffFOZmZm65ZZb9NOf/lQrV67UY489pqSkpD4cBoDYxTRjYDC6qou7gUBAkpSenh62/uWXX9aIESM0adIkVVVV6eOPP3ba6uvrNXnyZGVmZjrriouLFQwGdeTIkUu+T2dnp4LBYNgCYHBwcZMsMCj16gzKnwuFQlq2bJluu+02TZo0yVn/ve99T6NHj1Z2drYOHjyolStXqqmpSa+88ookye/3h4UTSc5rv99/yfeqrq7WmjVr+loqgGhGQAEGpT4HlPLych0+fFhvv/122PolS5Y4P0+ePFlZWVmaOXOmjh8/rptuuqlP71VVVaXKykrndTAYVE5OTt8KBxBVOIMCDE59usRTUVGhbdu26a233tKoUaMu27ewsFCSdOzYMUmS1+tVa2trWJ+Lr7/ovpXk5GS53e6wBcBgQUABBqNeBRRjjCoqKrRlyxbt3LlTY8aM+dJtGhsbJUlZWVmSJJ/Pp0OHDqmtrc3pU1NTI7fbrby8vN6UA2AwiCOgAINRry7xlJeXa+PGjXrttdc0bNgw554Rj8ejIUOG6Pjx49q4caPmzp2r4cOH6+DBg1q+fLluv/125efnS5JmzZqlvLw8LViwQGvXrpXf79eqVatUXl6u5OTk/j9CAFGNp6AAg1OvzqCsW7dOgUBAM2bMUFZWlrP88pe/lCQlJSXpzTff1KxZszRhwgQ9/PDDKi0t1datW519xMfHa9u2bYqPj5fP59Nf//Vfa+HChWHPTQEAhytOXOYBBp9enUExxly2PScnR3V1dV+6n9GjR+v111/vzVsDGKS4SRYYnPiSCwB2I6AAgxIBBYDVXOJR98BgREABYDfOoACDEgEFgN0IKMCgREABYDkCCjAYEVAAWO3TWTyEFGCwIaAAsBzhBBiMCCgA7MY9KMCgREABYDUe1AYMTgQUAHZz8W08wGBEQAFgNeIJMDgRUADYzcU/U8BgxF8+ALu5xI2ywCBEQAFgNRdnUIBBib98AHbj7AkwKBFQAFiNacbA4ERAAWA5AgowGBFQANiNMyjAoERAAWA3AgowKCVEugAAsS0UCikUCvV9+54r27anp0fd3d19fh+Xy6X4+Pg+bw+gfxFQAFxTL730kpYsWdLn7YelJOkXP7lb6cOGXLbfrYWFOvbHM31+n4KCAu3Zs6fP2wPoXwQUANeUMeaqzmx0dcVJ5sv79fR0X9X79PT09HlbAP2PgALAaiHzv+nkk56hOt01Sp09qYp3dSktsU3pif4IVgfgWiGgALCaMUZGRh/3DFPj2W+roydN3SZJcerRdXHndMOQwxo95GikywTQzwgoAKxmQkbdJkn17Xfpgklx1oeUoI9DaWrqKFSC64KMYbYPEEuYZgzAasZIu//n/4SFkz/Xo0QdPPctne1JH+DKAFxLBBQAVgvJXMFNspw9AWINAQWA1Yy5gik8AGIOAQWA1UIhcyWzjAHEGAIKAKsZI92W9ori1XXJdpd6dPPQ32lY/P8McGUArqVeBZR169YpPz9fbrdbbrdbPp9P27dvd9rPnz+v8vJyDR8+XKmpqSotLVVra2vYPpqbm1VSUqKUlBRlZGRoxYoVV/VwJQCxzUhKjvtYt13/fzU0/n/+f1AxcqlHyXHnNC7lXeVc955crr4/Th+AfXo1zXjUqFF68sknNW7cOBlj9NJLL+muu+7SgQMHdPPNN2v58uX6zW9+o82bN8vj8aiiokJ33323fv/730v69EmNJSUl8nq9euedd9TS0qKFCxcqMTFRTzzxxDU5QADRb8e+Y0od0qyPe96Tv/NGfRIapgTXBaUnnlIw6UMdkhTo6Ix0mQD6kctc5R1o6enpeuqpp3TPPfdo5MiR2rhxo+655x5J0vvvv6+JEyeqvr5e06dP1/bt23XHHXfo1KlTyszMlCStX79eK1eu1OnTp5WUlHRF7xkMBuXxePTAAw9c8TYAIqOpqUl1dXWRLuNLjRw5Ut/97ncjXQYQ0y5cuKANGzYoEAjI7XZftm+fH9TW09OjzZs3q6OjQz6fTw0NDerq6lJRUZHTZ8KECcrNzXUCSn19vSZPnuyEE0kqLi7W0qVLdeTIEX3ta1+75Ht1dnaqs/N//+8oGAxKkhYsWKDU1NS+HgKAAbB169aoCCgjRozQokWLIl0GENPOnTunDRs2XFHfXgeUQ4cOyefz6fz580pNTdWWLVuUl5enxsZGJSUlKS0tLax/Zmam/P5PvyvD7/eHhZOL7Rfbvkh1dbXWrFnzufVTp0790gQGILIOHz4c6RKuyNChQ3XrrbdGugwgpl08wXAlej2LZ/z48WpsbNTevXu1dOlSlZWV6ejRa/s9GFVVVQoEAs5y8uTJa/p+AAAgsnp9BiUpKUljx46VJBUUFGj//v169tlnde+99+rChQtqb28PO4vS2toqr9crSfJ6vdq3b1/Y/i7O8rnY51KSk5OVnJzc21IBAECUuurnoIRCIXV2dqqgoECJiYmqra112pqamtTc3CyfzydJ8vl8OnTokNra2pw+NTU1crvdysvLu9pSAABAjOjVGZSqqirNmTNHubm5Onv2rDZu3Khdu3bpjTfekMfj0aJFi1RZWan09HS53W499NBD8vl8mj59uiRp1qxZysvL04IFC7R27Vr5/X6tWrVK5eXlnCEBAACOXgWUtrY2LVy4UC0tLfJ4PMrPz9cbb7yh73znO5Kkp59+WnFxcSotLVVnZ6eKi4v1wgsvONvHx8dr27ZtWrp0qXw+n4YOHaqysjI9/vjj/XtUAAAgql31c1Ai4eJzUK5kHjWAyPr5z38eFdN3p06dqv3790e6DCCm9ebzm+/iAQAA1iGgAAAA6xBQAACAdQgoAADAOn3+Lh4AuBKjR4/WvHnzIl3Gl7r4AEoAdmAWDwAAGBDM4gEAAFGNgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOrwLKunXrlJ+fL7fbLbfbLZ/Pp+3btzvtM2bMkMvlClsefPDBsH00NzerpKREKSkpysjI0IoVK9Td3d0/RwMAAGJCQm86jxo1Sk8++aTGjRsnY4xeeukl3XXXXTpw4IBuvvlmSdLixYv1+OOPO9ukpKQ4P/f09KikpERer1fvvPOOWlpatHDhQiUmJuqJJ57op0MCAADRzmWMMVezg/T0dD311FNatGiRZsyYoVtuuUXPPPPMJftu375dd9xxh06dOqXMzExJ0vr167Vy5UqdPn1aSUlJV/SewWBQHo9HgUBAbrf7asoHAAADpDef332+B6Wnp0ebNm1SR0eHfD6fs/7ll1/WiBEjNGnSJFVVVenjjz922urr6zV58mQnnEhScXGxgsGgjhw58oXv1dnZqWAwGLYAAIDY1atLPJJ06NAh+Xw+nT9/XqmpqdqyZYvy8vIkSd/73vc0evRoZWdn6+DBg1q5cqWampr0yiuvSJL8fn9YOJHkvPb7/V/4ntXV1VqzZk1vSwUAAFGq1wFl/PjxamxsVCAQ0K9//WuVlZWprq5OeXl5WrJkidNv8uTJysrK0syZM3X8+HHddNNNfS6yqqpKlZWVzutgMKicnJw+7w8AANit15d4kpKSNHbsWBUUFKi6ulpTpkzRs88+e8m+hYWFkqRjx45Jkrxer1pbW8P6XHzt9Xq/8D2Tk5OdmUMXFwAAELuu+jkooVBInZ2dl2xrbGyUJGVlZUmSfD6fDh06pLa2NqdPTU2N3G63c5kIAACgV5d4qqqqNGfOHOXm5urs2bPauHGjdu3apTfeeEPHjx/Xxo0bNXfuXA0fPlwHDx7U8uXLdfvttys/P1+SNGvWLOXl5WnBggVau3at/H6/Vq1apfLyciUnJ1+TAwQAANGnVwGlra1NCxcuVEtLizwej/Lz8/XGG2/oO9/5jk6ePKk333xTzzzzjDo6OpSTk6PS0lKtWrXK2T4+Pl7btm3T0qVL5fP5NHToUJWVlYU9NwUAAOCqn4MSCTwHBQCA6DMgz0EBAAC4VggoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1EiJdQF8YYyRJwWAwwpUAAIArdfFz++Ln+OVEZUA5e/asJCknJyfClQAAgN46e/asPB7PZfu4zJXEGMuEQiE1NTUpLy9PJ0+elNvtjnRJUSsYDConJ4dx7AeMZf9hLPsH49h/GMv+YYzR2bNnlZ2drbi4y99lEpVnUOLi4vSVr3xFkuR2u/ll6QeMY/9hLPsPY9k/GMf+w1hevS87c3IRN8kCAADrEFAAAIB1ojagJCcna/Xq1UpOTo50KVGNcew/jGX/YSz7B+PYfxjLgReVN8kCAIDYFrVnUAAAQOwioAAAAOsQUAAAgHUIKAAAwDpRGVCef/553XDDDbruuutUWFioffv2Rbok6+zevVt33nmnsrOz5XK59Oqrr4a1G2P06KOPKisrS0OGDFFRUZE++OCDsD5nzpzR/Pnz5Xa7lZaWpkWLFuncuXMDeBSRV11drWnTpmnYsGHKyMjQvHnz1NTUFNbn/PnzKi8v1/Dhw5WamqrS0lK1traG9WlublZJSYlSUlKUkZGhFStWqLu7eyAPJaLWrVun/Px85yFXPp9P27dvd9oZw7578skn5XK5tGzZMmcd43llHnvsMblcrrBlwoQJTjvjGGEmymzatMkkJSWZn//85+bIkSNm8eLFJi0tzbS2tka6NKu8/vrr5u/+7u/MK6+8YiSZLVu2hLU/+eSTxuPxmFdffdX8x3/8h/mrv/orM2bMGPPJJ584fWbPnm2mTJli9uzZY373u9+ZsWPHmvvvv3+AjySyiouLzYsvvmgOHz5sGhsbzdy5c01ubq45d+6c0+fBBx80OTk5pra21rz77rtm+vTp5i/+4i+c9u7ubjNp0iRTVFRkDhw4YF5//XUzYsQIU1VVFYlDioh///d/N7/5zW/Mf/7nf5qmpibz4x//2CQmJprDhw8bYxjDvtq3b5+54YYbTH5+vvnBD37grGc8r8zq1avNzTffbFpaWpzl9OnTTjvjGFlRF1BuvfVWU15e7rzu6ekx2dnZprq6OoJV2e2zASUUChmv12ueeuopZ117e7tJTk42v/jFL4wxxhw9etRIMvv373f6bN++3bhcLvPHP/5xwGq3TVtbm5Fk6urqjDGfjltiYqLZvHmz0+e9994zkkx9fb0x5tOwGBcXZ/x+v9Nn3bp1xu12m87OzoE9AItcf/315l/+5V8Ywz46e/asGTdunKmpqTF/+Zd/6QQUxvPKrV692kyZMuWSbYxj5EXVJZ4LFy6ooaFBRUVFzrq4uDgVFRWpvr4+gpVFlxMnTsjv94eNo8fjUWFhoTOO9fX1SktL09SpU50+RUVFiouL0969ewe8ZlsEAgFJUnp6uiSpoaFBXV1dYWM5YcIE5ebmho3l5MmTlZmZ6fQpLi5WMBjUkSNHBrB6O/T09GjTpk3q6OiQz+djDPuovLxcJSUlYeMm8TvZWx988IGys7N14403av78+WpubpbEONogqr4s8E9/+pN6enrCfhkkKTMzU++//36Eqoo+fr9fki45jhfb/H6/MjIywtoTEhKUnp7u9BlsQqGQli1bpttuu02TJk2S9Ok4JSUlKS0tLazvZ8fyUmN9sW2wOHTokHw+n86fP6/U1FRt2bJFeXl5amxsZAx7adOmTfrDH/6g/fv3f66N38krV1hYqA0bNmj8+PFqaWnRmjVr9M1vflOHDx9mHC0QVQEFiKTy8nIdPnxYb7/9dqRLiUrjx49XY2OjAoGAfv3rX6usrEx1dXWRLivqnDx5Uj/4wQ9UU1Oj6667LtLlRLU5c+Y4P+fn56uwsFCjR4/Wr371Kw0ZMiSClUGKslk8I0aMUHx8/Ofuom5tbZXX641QVdHn4lhdbhy9Xq/a2trC2ru7u3XmzJlBOdYVFRXatm2b3nrrLY0aNcpZ7/V6deHCBbW3t4f1/+xYXmqsL7YNFklJSRo7dqwKCgpUXV2tKVOm6Nlnn2UMe6mhoUFtbW36+te/roSEBCUkJKiurk7PPfecEhISlJmZyXj2UVpamr761a/q2LFj/F5aIKoCSlJSkgoKClRbW+usC4VCqq2tlc/ni2Bl0WXMmDHyer1h4xgMBrV3715nHH0+n9rb29XQ0OD02blzp0KhkAoLCwe85kgxxqiiokJbtmzRzp07NWbMmLD2goICJSYmho1lU1OTmpubw8by0KFDYYGvpqZGbrdbeXl5A3MgFgqFQurs7GQMe2nmzJk6dOiQGhsbnWXq1KmaP3++8zPj2Tfnzp3T8ePHlZWVxe+lDSJ9l25vbdq0ySQnJ5sNGzaYo0ePmiVLlpi0tLSwu6jx6R3+Bw4cMAcOHDCSzD/90z+ZAwcOmP/+7/82xnw6zTgtLc289tpr5uDBg+auu+665DTjr33ta2bv3r3m7bffNuPGjRt004yXLl1qPB6P2bVrV9hUxI8//tjp8+CDD5rc3Fyzc+dO8+677xqfz2d8Pp/TfnEq4qxZs0xjY6PZsWOHGTly5KCaivjII4+Yuro6c+LECXPw4EHzyCOPGJfLZX77298aYxjDq/Xns3iMYTyv1MMPP2x27dplTpw4YX7/+9+boqIiM2LECNPW1maMYRwjLeoCijHG/OxnPzO5ubkmKSnJ3HrrrWbPnj2RLsk6b731lpH0uaWsrMwY8+lU45/85CcmMzPTJCcnm5kzZ5qmpqawfXz00Ufm/vvvN6mpqcbtdpvvf//75uzZsxE4msi51BhKMi+++KLT55NPPjF/+7d/a66//nqTkpJivvvd75qWlpaw/fzXf/2XmTNnjhkyZIgZMWKEefjhh01XV9cAH03k/M3f/I0ZPXq0SUpKMiNHjjQzZ850wokxjOHV+mxAYTyvzL333muysrJMUlKS+cpXvmLuvfdec+zYMaedcYwslzHGRObcDQAAwKVF1T0oAABgcCCgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6/w/7BIZOQNCijgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------------------------------------------------#\n",
    "env.reset()\n",
    "\n",
    "for t in count():\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())    \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    action = agent.select_action(state, policy_net)\n",
    "    state,reward,terminated,truncated,_ = env.step(action)\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(\"Complete!\")\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions:\n",
    "- Change up the network and other hyperparameters\n",
    "- Plot reward through time using matplotlib.pyplot\n",
    "- Implement A2C (requires research)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on completing the project! Check your results with the notebook in the `answers` directory and then send your final accuracy to your club/channel/mentor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
